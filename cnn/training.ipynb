{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce1b065-46ba-400e-8814-2f55d83ca773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd340202-0c6d-442e-bde2-db254ee26447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(grayscale: bool = False):\n",
    "    if grayscale:\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        train_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    test_transforms = train_transforms\n",
    "    return train_transforms, test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee1c947-01f1-44c4-90ad-38a430eac53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, images, targets, transform=None):\n",
    "    self.images = np.load(images)\n",
    "    self.targets = np.load(targets)\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.images[idx]\n",
    "    #print(image)\n",
    "    target = self.targets[idx]\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)  # Apply transformations if provided\n",
    "\n",
    "    image = torch.from_numpy(np.array(image)).float()  # Convert to PyTorch tensor (float32)\n",
    "    target = torch.from_numpy(np.array(target)).float()  # Convert to PyTorch tensor (float32)\n",
    "\n",
    "    return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef8c8be-2efc-4dcc-94ca-a07d830d56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTaskData:\n",
    "    def __init__(self, grayscale: bool = False) -> None:\n",
    "        self.grayscale = grayscale\n",
    "        self.image_folder_path: Path = Path(\"dataset/\")\n",
    "        self.train_transforms, self.test_transforms = get_transforms(grayscale)\n",
    "        self.trainloader = self.make_trainloader()\n",
    "        self.testloader = self.make_testloader()\n",
    "\n",
    "        \n",
    "    def make_trainloader(self):\n",
    "        train_dataset = RegressionDataset(\n",
    "            self.image_folder_path / 'train_images.npy',  # Assuming your images are in a single file\n",
    "            self.image_folder_path / 'train_targets.npy',\n",
    "            transform=self.train_transforms\n",
    "        )\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "        return trainloader\n",
    "    \n",
    "    def make_testloader(self):\n",
    "        test_dataset = RegressionDataset(\n",
    "            self.image_folder_path / 'test_images.npy',\n",
    "            self.image_folder_path / 'test_targets.npy',\n",
    "            transform=self.test_transforms\n",
    "        )\n",
    "        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "        return testloader\n",
    "    def visualize_image(self):\n",
    "        \"\"\"\n",
    "        This function visualizes a single image from the train set\n",
    "        \"\"\"\n",
    "        images, targets = next(iter(self.trainloader))\n",
    "        print(targets[0].shape)\n",
    "        print(images[0].shape)\n",
    "        if self.grayscale:\n",
    "            plt.imshow(images[0][0, :, :], cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(images[0].permute(1, 2, 0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f79304-b722-460d-9348-c0acad98c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_size: Tuple[int, int, int] = (4, 135, 135)):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.image_size[0], out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1_in = int(16*(image_size[1]//16)*(image_size[2]//8))\n",
    "        self.fc1 = nn.Linear(in_features=self.fc1_in, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.conv1(x)\n",
    "        # print('Size of tensor after each layer')\n",
    "        #print(f'conv1 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        #print(f'relu1 {x.size()}')\n",
    "        x = self.pool1(x)\n",
    "        #print(f'pool1 {x.size()}')\n",
    "        x = self.conv2(x)\n",
    "        #print(f'conv2 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        #print(f'relu2 {x.size()}')\n",
    "        x = self.pool2(x)\n",
    "        #print(f'pool2 {x.size()}')\n",
    "        x = self.conv3(x)\n",
    "        #print(f'conv3 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        #print(f'relu3 {x.size()}')\n",
    "        x = self.pool3(x)\n",
    "        #print(f'pool3 {x.size()}')\n",
    "        x = self.conv4(x)\n",
    "        #print(f'conv4 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        #print(f'relu4 {x.size()}')\n",
    "        x = self.pool4(x)\n",
    "        #print(f'pool4 {x.size()}')\n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        # print(f'view1 {x.size()}')\n",
    "        x = self.fc1(x)\n",
    "        # print(f'fc1 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        # print(f'relu2 {x.size()}')\n",
    "        x = self.fc2(x)\n",
    "        # print(f'fc2 {x.size()}')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b0e183a-dee1-4115-9a52-be7573835d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "def train_network(device, n_epochs: int = 10, image_size: Tuple[int, int, int] = (4, 135, 135)):\n",
    "    \"\"\"\n",
    "    This trains the network for a set number of epochs.\n",
    "    \"\"\"\n",
    "    if image_size[0] == 1:\n",
    "        grayscale = True\n",
    "    else:\n",
    "        grayscale = False\n",
    "    assert image_size[1] == image_size[2], 'Image size must be square'\n",
    "    \n",
    "    regression_task = RegressionTaskData(grayscale=grayscale)\n",
    "\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = CNNRegression(image_size=image_size)\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    #writer = SummaryWriter()\n",
    "    starttt = time.time()\n",
    "    best_loss = np.inf\n",
    "    best_weights = None\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time.time()\n",
    "        for i, (inputs, targets) in enumerate(regression_task.trainloader):\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #writer.add_scalar('Train Loss', loss.item(), i)\n",
    "\n",
    "            # Print training statistics\n",
    "            if i == len(regression_task.trainloader)//2:\n",
    "                print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i + 1}/{len(regression_task.trainloader)}], Loss: {loss.item():.7f}')\n",
    "        _, _, _, mean_distance_loss = evaluate_network(model, device, image_size = image_size)\n",
    "        if mean_distance_loss < best_loss:\n",
    "            best_loss = mean_distance_loss\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        duration = time.time()-start\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}] finished in {duration:.5f} seconds, mean distance loss: {mean_distance_loss:.7f} meters\\n-----')\n",
    "        history.append(mean_distance_loss)\n",
    "    #writer.close()\n",
    "    total_time = time.time() - starttt\n",
    "    print(f'Total time: {total_time:.5f} secs/{(total_time/60):.5f} mins\\nAverage time per epoch: {(total_time/n_epochs):.5f} seconds')\n",
    "    plt.plot(history)\n",
    "    \n",
    "    model.load_state_dict(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602ffef5-9fdd-419a-9de4-dd19df43fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename='4_135_135.pth'):\n",
    "    \"\"\"\n",
    "    After training the model, save it so we can use it later.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def load_model(image_size=(4, 135, 135), filename='4_135_135.pth'):\n",
    "    \"\"\"\n",
    "    Load the model from the saved state dictionary.\n",
    "    \"\"\"\n",
    "    model = CNNRegression(image_size)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model\n",
    "\n",
    "def evaluate_network(model, device, image_size: Tuple[int, int, int] = (4, 135, 135)):\n",
    "    \"\"\"\n",
    "    This evaluates the network on the test data.\n",
    "    \"\"\"\n",
    "    if image_size[0] == 1:\n",
    "        grayscale = True\n",
    "    else:\n",
    "        grayscale = False\n",
    "    assert image_size[1] == image_size[2], 'Image size must be square'\n",
    "    \n",
    "    regression_task = RegressionTaskData(grayscale=grayscale)\n",
    "    criterion = nn.MSELoss()\n",
    "    max_loss = 0\n",
    "    min_loss = np.inf\n",
    "    # Evaluate the model on the test data\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_distance_loss = 0\n",
    "        n_samples_total = 0\n",
    "        for inputs, targets in regression_task.testloader:\n",
    "            # Calculate the loss with the criterion we used in training\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # We are measuring the predicted distance away from the actual point, which is more useful to us than MSE loss\n",
    "            outputs_np = outputs.cpu().numpy()\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            distance_losses = []\n",
    "            for i in range(len(outputs_np)):\n",
    "                deltax = np.abs(outputs_np[i][0]-targets_np[i][0])\n",
    "                deltay = np.abs(outputs_np[i][1]-targets_np[i][1])\n",
    "\n",
    "                distance = np.sqrt(deltax*deltax + deltay*deltay)\n",
    "                distance_losses.append(distance)\n",
    "            if distance < min_loss:\n",
    "                min_loss = distance\n",
    "            if distance > max_loss:\n",
    "                max_loss = distance\n",
    "            total_distance_loss += sum(distance_losses)\n",
    "            n_samples_total += len(distance_losses)\n",
    "\n",
    "        mean_loss = total_loss / len(regression_task.testloader)\n",
    "        mean_distance_error = total_distance_loss / n_samples_total\n",
    "        return mean_loss, min_loss, max_loss, mean_distance_error\n",
    "        #print(f'Test Loss: {mean_loss:.4f}')\n",
    "        #print(f'Test mean distance error: {mean_distance_error:.4f} meters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b0d8689-3eb0-47f2-aa80-5d3b4d7b17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 4, 135, 135]             148\n",
      "         MaxPool2d-2            [-1, 4, 67, 67]               0\n",
      "            Conv2d-3            [-1, 8, 67, 67]             296\n",
      "         MaxPool2d-4            [-1, 8, 33, 33]               0\n",
      "            Conv2d-5           [-1, 16, 33, 33]           1,168\n",
      "         MaxPool2d-6           [-1, 16, 16, 16]               0\n",
      "            Conv2d-7           [-1, 32, 16, 16]           4,640\n",
      "         MaxPool2d-8             [-1, 32, 8, 8]               0\n",
      "            Linear-9                  [-1, 128]         262,272\n",
      "           Linear-10                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 268,782\n",
      "Trainable params: 268,782\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.28\n",
      "Forward/backward pass size (MB): 1.28\n",
      "Params size (MB): 1.03\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "vgg = CNNRegression()\n",
    "summary(vgg, (4, 135, 135))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef5c6a3-9cef-45fe-aef7-1d680b2774e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CNNRegression(\n",
      "  (conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m135\u001b[39m, \u001b[38;5;241m135\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39myscale(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(device, n_epochs, image_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m, in \u001b[0;36mCNNRegression.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# print(f'relu2 {x.size()}')\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# print(f'fc2 {x.size()}')\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "history = []\n",
    "# Train the model\n",
    "image_size: Tuple[int, int, int] = (4, 135, 135)\n",
    "model = train_network(device, 480, image_size=image_size)\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49174e12-4527-4b19-aa63-5118c6db5245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000005\n",
      "Test mean distance error: 0.002716 meters\n",
      "Min_loss: 0.000371 meters\n",
      "Max_loss: 0.007181 meters\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "filename = f'{image_size[0]}_{image_size[1]}_{image_size[2]}.pth'\n",
    "save_model(model, filename=filename)\n",
    "\n",
    "# Load the model\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mean_loss, min_loss, max_loss, mean_distance_error = evaluate_network(model, device, image_size=image_size)\n",
    "print(f'Test Loss: {mean_loss:.6f}\\nTest mean distance error: {mean_distance_error:.6f} meters\\nMin_loss: {min_loss:.6f} meters\\nMax_loss: {max_loss:.6f} meters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66315245-53c9-406e-89de-afab7d80c8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17274553 0.2613494 ] [0.17348922 0.26503036]\n"
     ]
    }
   ],
   "source": [
    "model = load_model(image_size=(4, 135, 135), filename=\"2.7mm.pth\")\n",
    "model.to(\"cpu\")\n",
    "images = np.load(\"images.npy\")\n",
    "xycoords = np.load(\"xycoords.npy\")\n",
    "index = 300\n",
    "tensorer = transforms.ToTensor()\n",
    "image = torch.from_numpy(np.array(tensorer(images[index]))).float()\n",
    "\n",
    "output = model(image.to(\"cpu\"))\n",
    "print(output.detach().numpy()[0], xycoords[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814861a4-6cb2-45e0-ba80-13068bafaa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
